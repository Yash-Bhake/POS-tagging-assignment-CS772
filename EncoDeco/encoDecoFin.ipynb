{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72bfbbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Maximum sentence length: 180\n",
      "Train size: 41284\n",
      "Validation size: 4588\n",
      "Test size: 11468\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "import random\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the dataset if you haven't already\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('universal_tagset', quiet=True)\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. Data Loading and Vocabulary Creation ---\n",
    "corpus = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "\n",
    "all_words = [word.lower() for sent in corpus for word, tag in sent]\n",
    "all_tags = [tag for sent in corpus for word, tag in sent]\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "word_to_idx = {word: i+2 for i, (word, _) in enumerate(word_counts.items())}\n",
    "word_to_idx['<PAD>'] = 0\n",
    "word_to_idx['<UNK>'] = 1\n",
    "\n",
    "tag_to_idx = {tag: i+1 for i, (tag, _) in enumerate(tag_counts.items())}\n",
    "tag_to_idx['<PAD>'] = 0\n",
    "\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "\n",
    "# --- 2. Convert Sentences to Indices and Pad ---\n",
    "sequences = []\n",
    "for sent in corpus:\n",
    "    word_indices = [word_to_idx.get(word.lower(), word_to_idx['<UNK>']) for word, tag in sent]\n",
    "    tag_indices = [tag_to_idx[tag] for word, tag in sent]\n",
    "    sequences.append((word_indices, tag_indices))\n",
    "\n",
    "MAX_LEN = max(len(s) for s, t in sequences)\n",
    "print(f\"Maximum sentence length: {MAX_LEN}\")\n",
    "\n",
    "def pad_sequences(sequences, max_len, word_pad_idx, tag_pad_idx):\n",
    "    padded_sents = []\n",
    "    padded_tags = []\n",
    "    for s, t in sequences:\n",
    "        padded_s = s + [word_pad_idx] * (max_len - len(s))\n",
    "        padded_t = t + [tag_pad_idx] * (max_len - len(t))\n",
    "        padded_sents.append(padded_s)\n",
    "        padded_tags.append(padded_t)\n",
    "    return np.array(padded_sents), np.array(padded_tags)\n",
    "\n",
    "padded_sents, padded_tags = pad_sequences(sequences, MAX_LEN, word_to_idx['<PAD>'], tag_to_idx['<PAD>'])\n",
    "\n",
    "# --- 3. Train, Validation, Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sents, padded_tags, test_size=0.2, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=SEED)\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test size: {len(X_test)}\")\n",
    "\n",
    "# --- 4. Create PyTorch Dataset and DataLoader ---\n",
    "class PosTaggingDataset(Dataset):\n",
    "    def __init__(self, sentences, tags):\n",
    "        self.sentences = torch.LongTensor(sentences)\n",
    "        self.tags = torch.LongTensor(tags)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.tags[idx]\n",
    "\n",
    "train_dataset = PosTaggingDataset(X_train, y_train)\n",
    "val_dataset = PosTaggingDataset(X_val, y_val)\n",
    "test_dataset = PosTaggingDataset(X_test, y_test)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97389d12",
   "metadata": {},
   "source": [
    "### fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc63b051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fastText model...\n",
      "Model loaded.\n",
      "fastText embedding matrix created.\n"
     ]
    }
   ],
   "source": [
    "# --- Create Embedding Matrix from Pre-trained fastText ---\n",
    "EMBEDDING_DIM = 300 # IMPORTANT: This must match the pre-trained model's dimension\n",
    "print(\"Loading fastText model...\")\n",
    "ft_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Create the embedding matrix for our vocabulary\n",
    "vocab_size = len(word_to_idx)\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_to_idx.items():\n",
    "    if word in ft_model:\n",
    "        embedding_matrix[i] = ft_model[word]\n",
    "\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix).to(device)\n",
    "print(\"fastText embedding matrix created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a842e",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3781a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The from-scratch LSTM cell remains the same\n",
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"A from-scratch implementation of a single LSTM cell.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_ih = nn.Linear(input_size, 4 * hidden_size)\n",
    "        self.linear_hh = nn.Linear(hidden_size, 4 * hidden_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        h_prev, c_prev = states\n",
    "        gates = self.linear_ih(x) + self.linear_hh(h_prev)\n",
    "        i, f, g, o = gates.chunk(4, dim=1)\n",
    "        \n",
    "        i_t = torch.sigmoid(i)\n",
    "        f_t = torch.sigmoid(f)\n",
    "        g_t = torch.tanh(g)\n",
    "        o_t = torch.sigmoid(o)\n",
    "        \n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t\n",
    "\n",
    "# --- RECTIFIED MODEL: BiLSTM Encoder + Linear Decoder ---\n",
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Encoder Part ---\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.embedding.weight.data.copy_(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # We build a BiLSTM from scratch using your LSTMCell\n",
    "        self.lstm_cells_fwd = nn.ModuleList([\n",
    "            LSTMCell(embedding_dim if i == 0 else hidden_dim, hidden_dim) \n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "        self.lstm_cells_bwd = nn.ModuleList([\n",
    "            LSTMCell(embedding_dim if i == 0 else hidden_dim, hidden_dim) \n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # --- Decoder Part ---\n",
    "        # The decoder is a linear layer mapping hidden states to tag probabilities\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim) # *2 for bidirectional\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [batch_size, seq_len]\n",
    "        batch_size, seq_len = src.shape\n",
    "        \n",
    "        # 1. ENCODING: Pass through embedding layer\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        input_seq = embedded.permute(1, 0, 2)\n",
    "        # input_seq: [seq_len, batch_size, embedding_dim]\n",
    "\n",
    "        # 2. ENCODING: Process with from-scratch BiLSTM\n",
    "        \n",
    "        # FORWARD PASS\n",
    "        fwd_outputs = []\n",
    "        h_fwd = [torch.zeros(batch_size, self.hidden_dim).to(device) for _ in range(self.n_layers)]\n",
    "        c_fwd = [torch.zeros(batch_size, self.hidden_dim).to(device) for _ in range(self.n_layers)]\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            input_t = input_seq[t]\n",
    "            for i, layer in enumerate(self.lstm_cells_fwd):\n",
    "                h_fwd[i], c_fwd[i] = layer(input_t, (h_fwd[i], c_fwd[i]))\n",
    "                input_t = self.dropout(h_fwd[i]) if i < self.n_layers - 1 else h_fwd[i]\n",
    "            fwd_outputs.append(h_fwd[-1])\n",
    "            \n",
    "        # BACKWARD PASS\n",
    "        bwd_outputs = []\n",
    "        h_bwd = [torch.zeros(batch_size, self.hidden_dim).to(device) for _ in range(self.n_layers)]\n",
    "        c_bwd = [torch.zeros(batch_size, self.hidden_dim).to(device) for _ in range(self.n_layers)]\n",
    "\n",
    "        for t in range(seq_len - 1, -1, -1):\n",
    "            input_t = input_seq[t]\n",
    "            for i, layer in enumerate(self.lstm_cells_bwd):\n",
    "                h_bwd[i], c_bwd[i] = layer(input_t, (h_bwd[i], c_bwd[i]))\n",
    "                input_t = self.dropout(h_bwd[i]) if i < self.n_layers - 1 else h_bwd[i]\n",
    "            bwd_outputs.append(h_bwd[-1])\n",
    "            \n",
    "        bwd_outputs.reverse()\n",
    "        \n",
    "        # Combine encoder outputs\n",
    "        fwd_outputs_tensor = torch.stack(fwd_outputs)\n",
    "        bwd_outputs_tensor = torch.stack(bwd_outputs)\n",
    "        lstm_outputs = torch.cat((fwd_outputs_tensor, bwd_outputs_tensor), dim=2)\n",
    "        \n",
    "        # 3. DECODING: Pass encoder outputs through the linear decoder\n",
    "        predictions = self.fc_out(self.dropout(lstm_outputs))\n",
    "        \n",
    "        return predictions.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746e97e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c1a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 646/646 [06:40<00:00,  1.61it/s]\n",
      "Evaluating: 100%|██████████| 72/72 [00:18<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 01 | Train Loss: 1.390 | Val. Loss: 0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 646/646 [06:54<00:00,  1.56it/s]\n",
      "Evaluating: 100%|██████████| 72/72 [00:23<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 02 | Train Loss: 0.797 | Val. Loss: 0.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 646/646 [07:21<00:00,  1.46it/s]\n",
      "Evaluating: 100%|██████████| 72/72 [00:22<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 03 | Train Loss: 0.713 | Val. Loss: 0.565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 646/646 [08:09<00:00,  1.32it/s]\n",
      "Evaluating: 100%|██████████| 72/72 [00:21<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 04 | Train Loss: 0.681 | Val. Loss: 0.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 646/646 [08:46<00:00,  1.23it/s]\n",
      "Evaluating: 100%|██████████| 72/72 [00:20<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 05 | Train Loss: 0.661 | Val. Loss: 0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|███████▏  | 463/646 [05:34<02:26,  1.25it/s]"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameters ---\n",
    "INPUT_DIM = len(word_to_idx)\n",
    "OUTPUT_DIM = len(tag_to_idx)\n",
    "EMBEDDING_DIM = 300       # From fastText\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 1              # A 1-layer BiLSTM is a strong baseline\n",
    "DROPOUT = 0.5             # Increased dropout for a more powerful model\n",
    "WORD_PAD_IDX = word_to_idx['<PAD>']\n",
    "TAG_PAD_IDX = tag_to_idx['<PAD>']\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# --- Model, Loss, and Optimizer Initialization ---\n",
    "model = BiLSTMPOSTagger(\n",
    "    INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, \n",
    "    N_LAYERS, DROPOUT, WORD_PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name and 'embedding' not in name:\n",
    "            nn.init.xavier_uniform_(param.data)\n",
    "        elif 'bias' in name:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TAG_PAD_IDX)\n",
    "\n",
    "# --- Simplified Training and Evaluation Loops ---\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg in tqdm(iterator, desc=\"Training\"):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # The forward pass now takes only the source sentence\n",
    "        output = model(src)\n",
    "        \n",
    "        # CORRECTED THIS LINE: Use .reshape() instead of .view()\n",
    "        output = output.reshape(-1, output.shape[-1])\n",
    "        trg = trg.reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in tqdm(iterator, desc=\"Evaluating\"):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            \n",
    "            output = model(src)\n",
    "            \n",
    "            # CORRECTED THIS LINE: Use .reshape() instead of .view()\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            trg = trg.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# --- Start Training ---\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'pos-tagger-model-fixed.pt')\n",
    "        \n",
    "    print(f'\\nEpoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5546f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('pos-tagger-model-fixed.pt'))\n",
    "\n",
    "def get_predictions(model, iterator, pad_idx):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for src, trg in tqdm(iterator, desc=\"Testing\"):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            \n",
    "            output = model(src)\n",
    "            preds = output.argmax(dim=2)\n",
    "            \n",
    "            for i in range(trg.shape[0]):\n",
    "                true_len = (trg[i] != pad_idx).sum()\n",
    "                true_tags_i = trg[i][:true_len].cpu().numpy()\n",
    "                pred_tags_i = preds[i][:true_len].cpu().numpy()\n",
    "                all_true.extend(true_tags_i)\n",
    "                all_preds.extend(pred_tags_i)\n",
    "                \n",
    "    return all_true, all_preds\n",
    "\n",
    "true_tags, pred_tags = get_predictions(model, test_loader, TAG_PAD_IDX)\n",
    "\n",
    "# Convert indices back to tag names\n",
    "true_labels = [idx_to_tag[t] for t in true_tags]\n",
    "pred_labels = [idx_to_tag[p] for p in pred_tags]\n",
    "tag_names = [tag for tag, i in tag_to_idx.items() if i != TAG_PAD_IDX]\n",
    "\n",
    "# --- 1. Classification Report ---\n",
    "report = classification_report(true_labels, pred_labels, labels=tag_names, zero_division=0)\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(report)\n",
    "\n",
    "# --- 2. Confusion Matrix ---\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels, labels=tag_names)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=tag_names, yticklabels=tag_names)\n",
    "plt.xlabel('Predicted Tags')\n",
    "plt.ylabel('True Tags')\n",
    "plt.title('Confusion Matrix for POS Tags')\n",
    "plt.show()\n",
    "\n",
    "# --- 3. Example Prediction Function (Updated) ---\n",
    "def tag_sentence(sentence, model, word_to_idx, idx_to_tag, device, max_len):\n",
    "    model.eval()\n",
    "    tokens = [word.lower() for word in sentence.split(' ')]\n",
    "    indices = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
    "    \n",
    "    padded_indices = indices + [word_to_idx['<PAD>']] * (max_len - len(indices))\n",
    "    src_tensor = torch.LongTensor(padded_indices).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(src_tensor)\n",
    "    \n",
    "    pred_indices = output.argmax(2).squeeze(0).cpu().numpy()\n",
    "    pred_tags = [idx_to_tag[i] for i in pred_indices[:len(tokens)]]\n",
    "    \n",
    "    return list(zip(sentence.split(' '), pred_tags))\n",
    "\n",
    "# Test a sentence\n",
    "test_sentence = \"the old man the boat\"\n",
    "tagged_sentence = tag_sentence(test_sentence, model, word_to_idx, idx_to_tag, device, MAX_LEN)\n",
    "print(f\"\\nExample tagging for: '{test_sentence}'\")\n",
    "print(tagged_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
