{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b6609d6",
   "metadata": {},
   "source": [
    "## POS Tagging - Encoder Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de538d",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea6f96ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nltk\n",
    "# from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import random\n",
    "import json\n",
    "# For evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the dataset if you haven't already\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('universal_tagset', quiet=True)\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867930e",
   "metadata": {},
   "source": [
    "### Data Loading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c70634d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length: 180\n",
      "Train size: 41284\n",
      "Validation size: 4588\n",
      "Test size: 11468\n"
     ]
    }
   ],
   "source": [
    "# Load the Brown corpus with the universal tagset\n",
    "corpus = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "\n",
    "# --- 1. Create Vocabularies ---\n",
    "all_words = [word.lower() for sent in corpus for word, tag in sent]\n",
    "all_tags = [tag for sent in corpus for word, tag in sent]\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "# Create word and tag to index mappings\n",
    "# Add special tokens: <PAD> for padding, <UNK> for unknown words\n",
    "word_to_idx = {word: i+2 for i, (word, _) in enumerate(word_counts.items())}\n",
    "word_to_idx['<PAD>'] = 0\n",
    "word_to_idx['<UNK>'] = 1\n",
    "\n",
    "tag_to_idx = {tag: i+1 for i, (tag, _) in enumerate(tag_counts.items())}\n",
    "tag_to_idx['<PAD>'] = 0\n",
    "\n",
    "# Create index to tag mapping for later use in evaluation\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "\n",
    "# --- 2. Convert Sentences to Indices and Pad ---\n",
    "def preprocess(corpus, word_to_idx, tag_to_idx):\n",
    "    sequences = []\n",
    "    for sent in corpus:\n",
    "        word_indices = [word_to_idx.get(word.lower(), word_to_idx['<UNK>']) for word, tag in sent]\n",
    "        tag_indices = [tag_to_idx[tag] for word, tag in sent]\n",
    "        sequences.append((word_indices, tag_indices))\n",
    "    return sequences\n",
    "\n",
    "sequences = preprocess(corpus, word_to_idx, tag_to_idx)\n",
    "\n",
    "# Find max sequence length for padding\n",
    "MAX_LEN = max(len(s) for s, t in sequences)\n",
    "print(f\"Maximum sentence length: {MAX_LEN}\")\n",
    "\n",
    "def pad_sequences(sequences, max_len, pad_idx):\n",
    "    padded_sents = []\n",
    "    padded_tags = []\n",
    "    for s, t in sequences:\n",
    "        padded_s = s + [pad_idx] * (max_len - len(s))\n",
    "        padded_t = t + [pad_idx] * (max_len - len(t))\n",
    "        padded_sents.append(padded_s)\n",
    "        padded_tags.append(padded_t)\n",
    "    return np.array(padded_sents), np.array(padded_tags)\n",
    "\n",
    "padded_sents, padded_tags = pad_sequences(sequences, MAX_LEN, word_to_idx['<PAD>'])\n",
    "\n",
    "# --- 3. Train, Validation, Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sents, padded_tags, test_size=0.2, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=SEED) # 0.1 * 0.8 = 0.08\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test size: {len(X_test)}\")\n",
    "\n",
    "\n",
    "# --- 4. Create PyTorch Dataset and DataLoader ---\n",
    "class PosTaggingDataset(Dataset):\n",
    "    def __init__(self, sentences, tags):\n",
    "        self.sentences = torch.LongTensor(sentences)\n",
    "        self.tags = torch.LongTensor(tags)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.tags[idx]\n",
    "\n",
    "train_dataset = PosTaggingDataset(X_train, y_train)\n",
    "val_dataset = PosTaggingDataset(X_val, y_val)\n",
    "test_dataset = PosTaggingDataset(X_test, y_test)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ec5cd3",
   "metadata": {},
   "source": [
    "### fastText Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dc801a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fastText model... (This may take a few minutes on first download)\n",
      "Model loaded.\n",
      "fastText embedding matrix created.\n"
     ]
    }
   ],
   "source": [
    "# --- Create Embedding Matrix from Pre-trained fastText ---\n",
    "\n",
    "EMBEDDING_DIM = 300 # IMPORTANT: This must match the pre-trained model's dimension\n",
    "# print(\"Loading fastText model... (This may take a few minutes on first download)\")\n",
    "ft_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# 2. Create the embedding matrix for our vocabulary\n",
    "vocab_size = len(word_to_idx)\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_to_idx.items():\n",
    "    if word in ft_model:\n",
    "        embedding_matrix[i] = ft_model[word]\n",
    "    # fastText can generate vectors for OOV words, but for simplicity\n",
    "    # we'll stick to the known vocab here. The real power is in its\n",
    "    # internal handling of word variations.\n",
    "\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix).to(device)\n",
    "print(\"fastText embedding matrix created.\")\n",
    "\n",
    "# IMPORTANT: Remember to update the HIDDEN_DIM and other model parameters\n",
    "# if changing the EMBEDDING_DIM from 100 to 300.\n",
    "# ...\n",
    "# ENC_EMB_DIM = EMBEDDING_DIM # This will now be 300\n",
    "# HIDDEN_DIM = 256 # You might want to increase this as well\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d257919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49817, 300])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95412e18",
   "metadata": {},
   "source": [
    "### LSTM Block\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "i_t &= \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "f_t &= \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "g_t &= \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "o_t &= \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "c_t &= f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "h_t &= o_t \\odot \\tanh(c_t)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "315e889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Class\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"A from-scratch implementation of a single LSTM cell.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Linear transformations for input and hidden state for all 4 gates\n",
    "        # This is more efficient than 4 separate Linear layers.\n",
    "        self.linear_ih = nn.Linear(input_size, 4 * hidden_size)\n",
    "        self.linear_hh = nn.Linear(hidden_size, 4 * hidden_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        # x: (batch_size, input_size)\n",
    "        # states: tuple (h_prev, c_prev)\n",
    "        # h_prev, c_prev: (batch_size, hidden_size)\n",
    "        h_prev, c_prev = states\n",
    "\n",
    "        # Compute all gate values at once\n",
    "        gates = self.linear_ih(x) + self.linear_hh(h_prev)\n",
    "        \n",
    "        # Split the gates into input, forget, cell, and output gates\n",
    "        # Each chunk has size (batch_size, hidden_size)\n",
    "        i, f, g, o = gates.chunk(4, dim=1)\n",
    "\n",
    "        # Apply activation functions\n",
    "        i_t = torch.sigmoid(i) # Input gate\n",
    "        f_t = torch.sigmoid(f) # Forget gate\n",
    "        g_t = torch.tanh(g)    # Cell gate (candidate cell state)\n",
    "        o_t = torch.sigmoid(o) # Output gate\n",
    "\n",
    "        # Update the cell state\n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        \n",
    "        # Update the hidden state\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc178ff8",
   "metadata": {},
   "source": [
    "### The Encoder\n",
    "\n",
    "The encoder takes an input sequence and processes it step-by-step with our LSTMCell, producing a final \"context\" vector (the last hidden and cell states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac6ad853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        # We can initialize with our Word2Vec weights\n",
    "        self.embedding.weight.data.copy_(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # We will use our custom LSTM cell\n",
    "        self.lstm_cells = nn.ModuleList([LSTMCell(emb_dim if i == 0 else hidden_dim, hidden_dim) for i in range(n_layers)])\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: (batch_size, seq_len)\n",
    "        batch_size, seq_len = src.shape\n",
    "        embedded = self.embedding(src)\n",
    "        # embedded: (batch_size, seq_len, emb_dim)\n",
    "        \n",
    "        # Initialize hidden and cell states as lists of tensors for each layer\n",
    "        h_states = [torch.zeros(batch_size, self.hidden_dim).to(device) for _ in range(self.n_layers)]\n",
    "        c_states = [torch.zeros(batch_size, self.hidden_dim).to(device) for _ in range(self.n_layers)]\n",
    "        \n",
    "        # Process sequence one token at a time\n",
    "        # We don't really need the outputs at each time step for the encoder\n",
    "        input_for_lstm = embedded.permute(1, 0, 2) # (seq_len, batch_size, emb_dim)\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            input_t = input_for_lstm[t] # (batch_size, emb_dim)\n",
    "            for i, layer in enumerate(self.lstm_cells):\n",
    "                # Update the states in the list (this is not an inplace operation)\n",
    "                h_states[i], c_states[i] = layer(input_t, (h_states[i], c_states[i]))\n",
    "                input_t = h_states[i] # Input to next layer is hidden state of current layer\n",
    "        \n",
    "        # Stack the lists to create the final context tensors\n",
    "        return torch.stack(h_states), torch.stack(c_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f85c71",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder takes the encoder's context and generates the output sequence (the POS tags) one tag at a time. It uses the previously predicted tag as input for the next prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e0c9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Tag embedding layer (the decoder's input are tags)\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.lstm_cells = nn.ModuleList([LSTMCell(emb_dim if i == 0 else hidden_dim, hidden_dim) for i in range(n_layers)])\n",
    "\n",
    "        # A linear layer to map the hidden state to the tag vocabulary\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input: (batch_size) - current tag token\n",
    "        # hidden, cell: (n_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        # Unbind the layers into a list of tensors\n",
    "        hidden_states = list(hidden.unbind(0))\n",
    "        cell_states = list(cell.unbind(0))\n",
    "\n",
    "        input = input.unsqueeze(0) # (1, batch_size)\n",
    "        embedded = self.embedding(input).squeeze(0) # (batch_size, emb_dim)\n",
    "        \n",
    "        input_for_layer = embedded\n",
    "        for i, layer in enumerate(self.lstm_cells):\n",
    "            hidden_states[i], cell_states[i] = layer(input_for_layer, (hidden_states[i], cell_states[i]))\n",
    "            input_for_layer = hidden_states[i]\n",
    "            \n",
    "        # Stack the updated states back into single tensors\n",
    "        h_new = torch.stack(hidden_states)\n",
    "        c_new = torch.stack(cell_states)\n",
    "\n",
    "        prediction = self.fc_out(h_new[-1]) # Use hidden state from the last layer for prediction\n",
    "        # prediction: (batch_size, output_dim)\n",
    "        \n",
    "        return prediction, h_new, c_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c02cd5",
   "metadata": {},
   "source": [
    "### The Full Encoder-Decoder (Seq2Seq) Model\n",
    "\n",
    "This class ties the Encoder and Decoder together. It includes \"teacher forcing,\" a technique where we feed the ground-truth tag to the decoder during training to stabilize learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15d8a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: (batch_size, src_len)\n",
    "        # trg: (batch_size, trg_len)\n",
    "        batch_size, trg_len = trg.shape\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # To store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encoder forward pass\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Decoder's first input is the <PAD> token (or a <SOS> token if we used one)\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # Decide whether to use teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1) # Get the predicted tag\n",
    "            \n",
    "            # If teacher forcing, use actual next token as next input\n",
    "            # otherwise, use predicted token\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "            \n",
    "        return outputs.permute(1, 0, 2) # (batch_size, seq_len, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980d788",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 646/646 [13:37<00:00,  1.27s/it]\n",
      "Evaluating: 100%|██████████| 72/72 [00:31<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 01\n",
      "\tTrain Loss: 2.030\n",
      "\t Val. Loss: 2.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 646/646 [14:01<00:00,  1.30s/it]\n",
      "Evaluating: 100%|██████████| 72/72 [00:23<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 02\n",
      "\tTrain Loss: 1.929\n",
      "\t Val. Loss: 2.118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 646/646 [16:14<00:00,  1.51s/it]\n",
      "Evaluating: 100%|██████████| 72/72 [00:27<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 03\n",
      "\tTrain Loss: 1.914\n",
      "\t Val. Loss: 2.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 72/646 [01:54<16:42,  1.75s/it]"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameters ---\n",
    "INPUT_DIM = len(word_to_idx)\n",
    "OUTPUT_DIM = len(tag_to_idx)\n",
    "ENC_EMB_DIM = EMBEDDING_DIM \n",
    "DEC_EMB_DIM = 100           \n",
    "HIDDEN_DIM = 256           \n",
    "N_LAYERS = 2\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "# --- Model Initialization ---\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM, N_LAYERS)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM, N_LAYERS)\n",
    "model = EncoderDecoder(encoder, decoder, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# --- Loss and Optimizer ---\n",
    "# We ignore the <PAD> token in the loss calculation\n",
    "PAD_IDX = tag_to_idx['<PAD>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# --- Training and Evaluation Loops ---\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg) in enumerate(tqdm(iterator, desc=\"Training\")):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg) # teacher_forcing_ratio is 0.5 by default\n",
    "        \n",
    "        # Reshape for CrossEntropyLoss: (N, C)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim) # Ignore first <PAD> prediction\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(tqdm(iterator, desc=\"Evaluating\")):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            \n",
    "            # Turn off teacher forcing for evaluation\n",
    "            output = model(src, trg, 0) \n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# --- Start Training ---\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'pos-tagger-model.pt')\n",
    "        \n",
    "    print(f'\\nEpoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d10f76",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb03cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('pos-tagger-model.pt'))\n",
    "\n",
    "def get_predictions(model, iterator):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(tqdm(iterator, desc=\"Testing\")):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            \n",
    "            output = model(src, trg, 0) # No teacher forcing\n",
    "            \n",
    "            # Get predictions (argmax) and flatten\n",
    "            preds = output.argmax(dim=2)\n",
    "            \n",
    "            # Flatten and ignore padding for evaluation\n",
    "            for j in range(trg.shape[0]):\n",
    "                true_tags = trg[j, 1:] # Ignore first tag\n",
    "                pred_tags = preds[j, 1:]\n",
    "                \n",
    "                # Find where the actual sentence ends (before padding)\n",
    "                true_len = (true_tags != PAD_IDX).sum()\n",
    "                \n",
    "                all_true.extend(true_tags[:true_len].cpu().numpy())\n",
    "                all_preds.extend(pred_tags[:true_len].cpu().numpy())\n",
    "                \n",
    "    return all_true, all_preds\n",
    "\n",
    "true_tags, pred_tags = get_predictions(model, test_loader)\n",
    "\n",
    "# Convert indices back to tag names\n",
    "true_labels = [idx_to_tag[t] for t in true_tags]\n",
    "pred_labels = [idx_to_tag[p] for p in pred_tags]\n",
    "tag_names = [tag for tag, i in tag_to_idx.items() if i != PAD_IDX]\n",
    "\n",
    "# --- 1. Classification Report (Precision, Recall, F1-Score) ---\n",
    "report = classification_report(true_labels, pred_labels, labels=tag_names, zero_division=0)\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(report)\n",
    "\n",
    "# --- 2. Confusion Matrix ---\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels, labels=tag_names)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=tag_names, yticklabels=tag_names)\n",
    "plt.xlabel('Predicted Tags')\n",
    "plt.ylabel('True Tags')\n",
    "plt.title('Confusion Matrix for POS Tags')\n",
    "plt.show()\n",
    "\n",
    "# Example of a prediction\n",
    "def tag_sentence(sentence, model, word_to_idx, idx_to_tag, device, max_len):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and numericalize\n",
    "    tokens = [word.lower() for word in sentence.split(' ')]\n",
    "    indices = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
    "    \n",
    "    # Pad\n",
    "    padded_indices = indices + [word_to_idx['<PAD>']] * (max_len - len(indices))\n",
    "    src_tensor = torch.LongTensor(padded_indices).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Create a dummy target tensor\n",
    "    trg_tensor = torch.zeros_like(src_tensor).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(src_tensor, trg_tensor, 0)\n",
    "    \n",
    "    pred_tags_indices = output.argmax(2).squeeze(0).cpu().numpy()\n",
    "    pred_tags = [idx_to_tag[i] for i in pred_tags_indices[1:len(tokens)+1]]\n",
    "    \n",
    "    return list(zip(tokens, pred_tags))\n",
    "    \n",
    "# Test a sentence\n",
    "test_sentence = \"the old man the boat\"\n",
    "tagged_sentence = tag_sentence(test_sentence, model, word_to_idx, idx_to_tag, device, MAX_LEN)\n",
    "print(f\"\\nExample tagging for: '{test_sentence}'\")\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e59b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ADD THIS CODE TO THE END OF YOUR TRAINING SCRIPT ---\n",
    "\n",
    "print(\"\\n--- Saving artifacts for deployment ---\")\n",
    "\n",
    "# 1. Save the word-to-index and tag-to-index mappings\n",
    "artifacts = {\n",
    "    'word_to_idx': word_to_idx,\n",
    "    'tag_to_idx': tag_to_idx,\n",
    "    'MAX_LEN': MAX_LEN\n",
    "}\n",
    "\n",
    "with open('artifacts.json', 'w') as f:\n",
    "    json.dump(artifacts, f, indent=4)\n",
    "\n",
    "# 2. The model is already saved as 'pos-tagger-model.pt' in the training loop\n",
    "\n",
    "print(\"Artifacts saved to 'artifacts.json'\")\n",
    "print(\"Model saved to 'pos-tagger-model.pt'\")\n",
    "print(\"Ready for Gradio deployment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
