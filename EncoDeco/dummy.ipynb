{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "addc3e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Maximum sentence length: 180\n",
      "Train size: 41284\n",
      "Validation size: 4588\n",
      "Test size: 11468\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "import random\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the dataset if you haven't already\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('universal_tagset', quiet=True)\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. Data Loading and Vocabulary Creation ---\n",
    "corpus = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "\n",
    "all_words = [word.lower() for sent in corpus for word, tag in sent]\n",
    "all_tags = [tag for sent in corpus for word, tag in sent]\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "word_to_idx = {word: i+2 for i, (word, _) in enumerate(word_counts.items())}\n",
    "word_to_idx['<PAD>'] = 0\n",
    "word_to_idx['<UNK>'] = 1\n",
    "\n",
    "tag_to_idx = {tag: i+1 for i, (tag, _) in enumerate(tag_counts.items())}\n",
    "tag_to_idx['<PAD>'] = 0\n",
    "\n",
    "idx_to_tag = {i: tag for tag, i in tag_to_idx.items()}\n",
    "\n",
    "# --- 2. Convert Sentences to Indices and Pad ---\n",
    "sequences = []\n",
    "for sent in corpus:\n",
    "    word_indices = [word_to_idx.get(word.lower(), word_to_idx['<UNK>']) for word, tag in sent]\n",
    "    tag_indices = [tag_to_idx[tag] for word, tag in sent]\n",
    "    sequences.append((word_indices, tag_indices))\n",
    "\n",
    "MAX_LEN = max(len(s) for s, t in sequences)\n",
    "print(f\"Maximum sentence length: {MAX_LEN}\")\n",
    "\n",
    "def pad_sequences(sequences, max_len, word_pad_idx, tag_pad_idx):\n",
    "    padded_sents = []\n",
    "    padded_tags = []\n",
    "    for s, t in sequences:\n",
    "        padded_s = s + [word_pad_idx] * (max_len - len(s))\n",
    "        padded_t = t + [tag_pad_idx] * (max_len - len(t))\n",
    "        padded_sents.append(padded_s)\n",
    "        padded_tags.append(padded_t)\n",
    "    return np.array(padded_sents), np.array(padded_tags)\n",
    "\n",
    "padded_sents, padded_tags = pad_sequences(sequences, MAX_LEN, word_to_idx['<PAD>'], tag_to_idx['<PAD>'])\n",
    "\n",
    "# --- 3. Train, Validation, Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sents, padded_tags, test_size=0.2, random_state=SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=SEED)\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test size: {len(X_test)}\")\n",
    "\n",
    "# --- 4. Create PyTorch Dataset and DataLoader ---\n",
    "class PosTaggingDataset(Dataset):\n",
    "    def __init__(self, sentences, tags):\n",
    "        self.sentences = torch.LongTensor(sentences)\n",
    "        self.tags = torch.LongTensor(tags)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.tags[idx]\n",
    "\n",
    "train_dataset = PosTaggingDataset(X_train, y_train)\n",
    "val_dataset = PosTaggingDataset(X_val, y_val)\n",
    "test_dataset = PosTaggingDataset(X_test, y_test)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f85d1730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN   - 275558 instances\n",
      "VERB   - 182750 instances\n",
      ".      - 147565 instances\n",
      "ADP    - 144766 instances\n",
      "DET    - 137019 instances\n",
      "ADJ    -  83721 instances\n",
      "ADV    -  56239 instances\n",
      "PRON   -  49334 instances\n",
      "CONJ   -  38151 instances\n",
      "PRT    -  29829 instances\n",
      "NUM    -  14874 instances\n",
      "X      -   1386 instances\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Make sure 'all_tags' from your preprocessing step is available\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "# Print the counts of each tag, sorted by frequency\n",
    "for tag, count in tag_counts.most_common():\n",
    "    print(f\"{tag:<6} - {count:>6} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd77c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved successfully to 'artifacts.json'!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Make sure these variables are defined in your notebook's memory\n",
    "# from the preprocessing steps.\n",
    "artifacts = {\n",
    "    'word_to_idx': word_to_idx,\n",
    "    'tag_to_idx': tag_to_idx,\n",
    "    'MAX_LEN': MAX_LEN\n",
    "}\n",
    "\n",
    "with open('artifacts.json', 'w') as f:\n",
    "    json.dump(artifacts, f, indent=4)\n",
    "\n",
    "print(\"Artifacts saved successfully to 'artifacts.json'!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
