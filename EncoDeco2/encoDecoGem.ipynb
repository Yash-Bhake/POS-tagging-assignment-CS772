{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bbe5c40",
   "metadata": {},
   "source": [
    "pos tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6eeb9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/yashbhake/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/yashbhake/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4461\n",
      "Tag set size: 16\n",
      "\n",
      "Sample processed sentence:\n",
      "Original: [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n",
      "Tensor: (tensor([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
      "        21, 22, 23, 24, 25, 26, 27, 28,  3]), tensor([2, 4, 5, 5, 6, 5, 7, 5, 4, 5, 8, 5, 6, 5, 5, 7, 9, 4, 5, 9, 8, 4, 5, 7,\n",
      "        5, 9, 3]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Download necessary NLTK data (if you haven't already)\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# 1. Load and prepare the dataset\n",
    "# We get sentences tagged with universal POS tags\n",
    "tagged_sents = brown.tagged_sents(tagset='universal')\n",
    "\n",
    "# For simplicity, let's use a smaller subset of the data\n",
    "# Using the full dataset would take much longer to train\n",
    "data = tagged_sents[:5000]  # Use first 5000 sentences for this example\n",
    "\n",
    "\n",
    "# 2. Create vocabulary and tag mappings\n",
    "word_counts = defaultdict(int)\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "for sent in data:\n",
    "    for word, tag in sent:\n",
    "        word_counts[word.lower()] += 1\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "# Create word_to_ix and tag_to_ix dictionaries\n",
    "# We'll add special tokens for handling sequences\n",
    "# <PAD>: Padding, <UNK>: Unknown words, <SOS>: Start of Sequence, <EOS>: End of Sequence\n",
    "word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1, \"<SOS>\": 2, \"<EOS>\": 3}\n",
    "for word, count in word_counts.items():\n",
    "    # A simple way to filter out rare words\n",
    "    if count > 2: \n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1, \"<SOS>\": 2, \"<EOS>\": 3}\n",
    "for tag in tag_counts:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "# Create inverse mapping for later use\n",
    "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "\n",
    "# 3. Convert sentences to integer sequences\n",
    "def sentence_to_indices(sentence, word_to_ix):\n",
    "    return [word_to_ix.get(word.lower(), word_to_ix[\"<UNK>\"]) for word in sentence]\n",
    "\n",
    "def tags_to_indices(tags, tag_to_ix):\n",
    "    return [tag_to_ix[tag] for tag in tags]\n",
    "\n",
    "# Process the entire dataset\n",
    "processed_data = []\n",
    "for sent in data:\n",
    "    words = [word for word, tag in sent]\n",
    "    tags = [tag for word, tag in sent]\n",
    "    \n",
    "    # Add start and end tokens\n",
    "    word_indices = [word_to_ix[\"<SOS>\"]] + sentence_to_indices(words, word_to_ix) + [word_to_ix[\"<EOS>\"]]\n",
    "    tag_indices = [tag_to_ix[\"<SOS>\"]] + tags_to_indices(tags, tag_to_ix) + [tag_to_ix[\"<EOS>\"]]\n",
    "    \n",
    "    processed_data.append((torch.tensor(word_indices), torch.tensor(tag_indices)))\n",
    "\n",
    "print(f\"Vocabulary size: {len(word_to_ix)}\")\n",
    "print(f\"Tag set size: {len(tag_to_ix)}\")\n",
    "print(\"\\nSample processed sentence:\")\n",
    "print(f\"Original: {data[0]}\")\n",
    "print(f\"Tensor: {processed_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ce514f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_i = nn.Linear(input_size, hidden_size)\n",
    "        self.W_f = nn.Linear(input_size, hidden_size)\n",
    "        self.W_o = nn.Linear(input_size, hidden_size)\n",
    "        self.W_g = nn.Linear(input_size, hidden_size)\n",
    "        self.U_i = nn.Linear(hidden_size, hidden_size)\n",
    "        self.U_f = nn.Linear(hidden_size, hidden_size)\n",
    "        self.U_o = nn.Linear(hidden_size, hidden_size)\n",
    "        self.U_g = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        h_prev, c_prev = states\n",
    "        f_t = torch.sigmoid(self.W_f(x) + self.U_f(h_prev))\n",
    "        i_t = torch.sigmoid(self.W_i(x) + self.U_i(h_prev))\n",
    "        o_t = torch.sigmoid(self.W_o(x) + self.U_o(h_prev))\n",
    "        g_t = torch.tanh(self.W_g(x) + self.U_g(h_prev))\n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        return h_t, c_t\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_cell = LSTMCell(embedding_dim, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        h = torch.zeros(1, self.hidden_size)\n",
    "        c = torch.zeros(1, self.hidden_size)\n",
    "        for i in range(embedded.shape[0]):\n",
    "            h, c = self.lstm_cell(embedded[i].unsqueeze(0), (h, c))\n",
    "        return h, c\n",
    "\n",
    "# THIS IS THE CORRECTED CLASS\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tag_size, embedding_dim, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(tag_size, embedding_dim)\n",
    "        self.lstm_cell = LSTMCell(embedding_dim, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, tag_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        # The buggy \".unsqueeze(0)\" has been REMOVED here.\n",
    "        embedded = self.embedding(x)\n",
    "        h, c = self.lstm_cell(embedded, states)\n",
    "        output = self.fc(h)\n",
    "        return output, (h, c)\n",
    "\n",
    "# THIS IS THE CORRECTED CLASS\n",
    "class Seq2SeqPOSTagger(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2SeqPOSTagger, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        batch_size = 1\n",
    "        target_len = len(target)\n",
    "        target_vocab_size = len(tag_to_ix)\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size)\n",
    "        h, c = self.encoder(source)\n",
    "        # We ensure the input is consistently shaped with .unsqueeze(0)\n",
    "        x = target[0].unsqueeze(0)\n",
    "        for t in range(1, target_len):\n",
    "            output, (h, c) = self.decoder(x, (h, c))\n",
    "            outputs[t] = output\n",
    "            x = target[t].unsqueeze(0)\n",
    "        return outputs.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73da1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Step 500/5000, Loss: 1.7163\n",
      "Epoch 1/10, Step 1000/5000, Loss: 1.6239\n",
      "Epoch 1/10, Step 1500/5000, Loss: 1.5945\n",
      "Epoch 1/10, Step 2000/5000, Loss: 1.5635\n",
      "Epoch 1/10, Step 2500/5000, Loss: 1.5370\n",
      "Epoch 1/10, Step 3000/5000, Loss: 1.5241\n",
      "Epoch 1/10, Step 3500/5000, Loss: 1.5189\n",
      "Epoch 1/10, Step 4000/5000, Loss: 1.5146\n",
      "Epoch 1/10, Step 4500/5000, Loss: 1.5159\n",
      "--- End of Epoch 1, Average Loss: 1.5115 ---\n",
      "Epoch 2/10, Step 500/5000, Loss: 1.4354\n",
      "Epoch 2/10, Step 1000/5000, Loss: 1.4120\n",
      "Epoch 2/10, Step 1500/5000, Loss: 1.4193\n",
      "Epoch 2/10, Step 2000/5000, Loss: 1.4159\n",
      "Epoch 2/10, Step 2500/5000, Loss: 1.4075\n",
      "Epoch 2/10, Step 3000/5000, Loss: 1.4108\n",
      "Epoch 2/10, Step 3500/5000, Loss: 1.4160\n",
      "Epoch 2/10, Step 4000/5000, Loss: 1.4217\n",
      "Epoch 2/10, Step 4500/5000, Loss: 1.4332\n",
      "--- End of Epoch 2, Average Loss: 1.4350 ---\n",
      "Epoch 3/10, Step 500/5000, Loss: 1.4078\n",
      "Epoch 3/10, Step 1000/5000, Loss: 1.3842\n",
      "Epoch 3/10, Step 1500/5000, Loss: 1.3895\n",
      "Epoch 3/10, Step 2000/5000, Loss: 1.3829\n",
      "Epoch 3/10, Step 2500/5000, Loss: 1.3739\n",
      "Epoch 3/10, Step 3000/5000, Loss: 1.3746\n",
      "Epoch 3/10, Step 3500/5000, Loss: 1.3762\n",
      "Epoch 3/10, Step 4000/5000, Loss: 1.3791\n",
      "Epoch 3/10, Step 4500/5000, Loss: 1.3860\n",
      "--- End of Epoch 3, Average Loss: 1.3858 ---\n",
      "Epoch 4/10, Step 500/5000, Loss: 1.3286\n",
      "Epoch 4/10, Step 1000/5000, Loss: 1.3031\n",
      "Epoch 4/10, Step 1500/5000, Loss: 1.3107\n",
      "Epoch 4/10, Step 2000/5000, Loss: 1.3079\n",
      "Epoch 4/10, Step 2500/5000, Loss: 1.3014\n",
      "Epoch 4/10, Step 3000/5000, Loss: 1.3047\n",
      "Epoch 4/10, Step 3500/5000, Loss: 1.3067\n",
      "Epoch 4/10, Step 4000/5000, Loss: 1.3144\n",
      "Epoch 4/10, Step 4500/5000, Loss: 1.3251\n",
      "--- End of Epoch 4, Average Loss: 1.3273 ---\n",
      "Epoch 5/10, Step 500/5000, Loss: 1.2915\n",
      "Epoch 5/10, Step 1000/5000, Loss: 1.2730\n",
      "Epoch 5/10, Step 1500/5000, Loss: 1.2786\n",
      "Epoch 5/10, Step 2000/5000, Loss: 1.2769\n",
      "Epoch 5/10, Step 2500/5000, Loss: 1.2717\n",
      "Epoch 5/10, Step 3000/5000, Loss: 1.2745\n",
      "Epoch 5/10, Step 3500/5000, Loss: 1.2780\n",
      "Epoch 5/10, Step 4000/5000, Loss: 1.2856\n",
      "Epoch 5/10, Step 4500/5000, Loss: 1.2950\n",
      "--- End of Epoch 5, Average Loss: 1.2981 ---\n",
      "Epoch 6/10, Step 500/5000, Loss: 1.2789\n",
      "Epoch 6/10, Step 1000/5000, Loss: 1.2574\n",
      "Epoch 6/10, Step 1500/5000, Loss: 1.2590\n",
      "Epoch 6/10, Step 2000/5000, Loss: 1.2581\n",
      "Epoch 6/10, Step 2500/5000, Loss: 1.2521\n",
      "Epoch 6/10, Step 3000/5000, Loss: 1.2572\n",
      "Epoch 6/10, Step 3500/5000, Loss: 1.2597\n",
      "Epoch 6/10, Step 4000/5000, Loss: 1.2675\n",
      "Epoch 6/10, Step 4500/5000, Loss: 1.2775\n",
      "--- End of Epoch 6, Average Loss: 1.2809 ---\n",
      "Epoch 7/10, Step 500/5000, Loss: 1.2596\n",
      "Epoch 7/10, Step 1000/5000, Loss: 1.2387\n",
      "Epoch 7/10, Step 1500/5000, Loss: 1.2456\n",
      "Epoch 7/10, Step 2000/5000, Loss: 1.2430\n",
      "Epoch 7/10, Step 2500/5000, Loss: 1.2382\n",
      "Epoch 7/10, Step 3000/5000, Loss: 1.2450\n",
      "Epoch 7/10, Step 3500/5000, Loss: 1.2474\n",
      "Epoch 7/10, Step 4000/5000, Loss: 1.2547\n",
      "Epoch 7/10, Step 4500/5000, Loss: 1.2642\n",
      "--- End of Epoch 7, Average Loss: 1.2679 ---\n",
      "Epoch 8/10, Step 500/5000, Loss: 1.2387\n",
      "Epoch 8/10, Step 1000/5000, Loss: 1.2191\n",
      "Epoch 8/10, Step 1500/5000, Loss: 1.2268\n",
      "Epoch 8/10, Step 2000/5000, Loss: 1.2225\n",
      "Epoch 8/10, Step 2500/5000, Loss: 1.2158\n",
      "Epoch 8/10, Step 3000/5000, Loss: 1.2214\n",
      "Epoch 8/10, Step 3500/5000, Loss: 1.2254\n",
      "Epoch 8/10, Step 4000/5000, Loss: 1.2339\n",
      "Epoch 8/10, Step 4500/5000, Loss: 1.2436\n",
      "--- End of Epoch 8, Average Loss: 1.2473 ---\n",
      "Epoch 9/10, Step 500/5000, Loss: 1.2379\n",
      "Epoch 9/10, Step 1000/5000, Loss: 1.2106\n",
      "Epoch 9/10, Step 1500/5000, Loss: 1.2207\n",
      "Epoch 9/10, Step 2000/5000, Loss: 1.2163\n",
      "Epoch 9/10, Step 2500/5000, Loss: 1.2105\n",
      "Epoch 9/10, Step 3000/5000, Loss: 1.2149\n",
      "Epoch 9/10, Step 3500/5000, Loss: 1.2169\n",
      "Epoch 9/10, Step 4000/5000, Loss: 1.2245\n",
      "Epoch 9/10, Step 4500/5000, Loss: 1.2335\n",
      "--- End of Epoch 9, Average Loss: 1.2369 ---\n",
      "Epoch 10/10, Step 500/5000, Loss: 1.2117\n",
      "Epoch 10/10, Step 1000/5000, Loss: 1.1886\n",
      "Epoch 10/10, Step 1500/5000, Loss: 1.2006\n",
      "Epoch 10/10, Step 2000/5000, Loss: 1.1995\n",
      "Epoch 10/10, Step 2500/5000, Loss: 1.1931\n",
      "Epoch 10/10, Step 3000/5000, Loss: 1.1982\n",
      "Epoch 10/10, Step 3500/5000, Loss: 1.2014\n",
      "Epoch 10/10, Step 4000/5000, Loss: 1.2116\n",
      "Epoch 10/10, Step 4500/5000, Loss: 1.2217\n",
      "--- End of Epoch 10, Average Loss: 1.2256 ---\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "# Model Initialization\n",
    "encoder = Encoder(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM)\n",
    "decoder = Decoder(len(tag_to_ix), EMBEDDING_DIM, HIDDEN_DIM)\n",
    "model = Seq2SeqPOSTagger(encoder, decoder)\n",
    "\n",
    "# Loss and Optimizer\n",
    "# We ignore the <PAD> token in our loss calculation\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tag_to_ix[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for i, (sentence_in, tags_out) in enumerate(processed_data):\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get model predictions\n",
    "        output = model(sentence_in, tags_out)\n",
    "        \n",
    "        # Reshape for CrossEntropyLoss:\n",
    "        # output should be (N, C) where C is number of classes\n",
    "        # tags_out should be (N)\n",
    "        # We skip the <SOS> token in our comparison\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        tags_out = tags_out[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, tags_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % 500 == 0 and i > 0:\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}, Step {i}/{len(processed_data)}, Loss: {total_loss / (i+1):.4f}\")\n",
    "\n",
    "    print(f\"--- End of Epoch {epoch+1}, Average Loss: {total_loss / len(processed_data):.4f} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "519407ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 5000\n",
      "Test samples: 1000\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have these libraries. If not, run: pip install scikit-learn tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm import tqdm # Provides a nice progress bar\n",
    "\n",
    "# Split the data you loaded and processed in the first script\n",
    "# We'll use 80% for training and 20% for testing\n",
    "split_point = int(len(processed_data) * 0.8)\n",
    "# We assume the model was trained on the first part\n",
    "train_data = processed_data[:split_point] \n",
    "# And we'll test on the second, unseen part\n",
    "test_data = processed_data[split_point:]\n",
    "\n",
    "print(f\"Total samples: {len(processed_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "\n",
    "# We need an inverse mapping from index to word for the evaluation function\n",
    "ix_to_word = {v: k for k, v in word_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966cff63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [00:04<00:00, 223.16it/s]\n",
      "/Users/yashbhake/Desktop/IIT B/Acads/SEM-7/CS772 Deep Learning for NLP/POS-tagging-assignment-CS772/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Overall Model Accuracy: 28.78%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       0.59      0.48      0.53      2617\n",
      "         ADJ       0.31      0.02      0.03      1660\n",
      "         ADP       0.17      0.29      0.21      2691\n",
      "         ADV       0.24      0.02      0.04       925\n",
      "        CONJ       0.29      0.08      0.12       630\n",
      "         DET       0.19      0.37      0.25      2490\n",
      "        NOUN       0.34      0.41      0.37      5709\n",
      "         NUM       0.00      0.00      0.00       348\n",
      "        PRON       0.76      0.17      0.28       620\n",
      "         PRT       0.50      0.00      0.00       530\n",
      "        VERB       0.26      0.20      0.23      3271\n",
      "           X       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.29     21522\n",
      "   macro avg       0.30      0.17      0.17     21522\n",
      "weighted avg       0.32      0.29      0.27     21522\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashbhake/Desktop/IIT B/Acads/SEM-7/CS772 Deep Learning for NLP/POS-tagging-assignment-CS772/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/yashbhake/Desktop/IIT B/Acads/SEM-7/CS772 Deep Learning for NLP/POS-tagging-assignment-CS772/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_data, word_to_ix, ix_to_word, ix_to_tag):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test data and prints performance metrics.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout, etc.)\n",
    "    \n",
    "    all_predicted_tags = []\n",
    "    all_true_tags = []\n",
    "\n",
    "    # Loop through the test data with a progress bar\n",
    "    for sentence_indices, tag_indices in tqdm(test_data, desc=\"Evaluating\"):\n",
    "        \n",
    "        # 1. Get the true tags, ignoring <SOS> and <EOS> tokens\n",
    "        true_tags = [ix_to_tag[ix.item()] for ix in tag_indices[1:-1]]\n",
    "        \n",
    "        # 2. Convert the input sentence indices back to a string\n",
    "        words = [ix_to_word.get(ix.item(), \"<UNK>\") for ix in sentence_indices[1:-1]]\n",
    "        sentence_str = \" \".join(words)\n",
    "        \n",
    "        # 3. Get model's prediction for the sentence\n",
    "        with torch.no_grad():\n",
    "            predicted_tags = tag_sentence(sentence_str, model, word_to_ix, ix_to_tag)\n",
    "        \n",
    "        # 4. Align predicted and true tags in case of length mismatch\n",
    "        # This can happen if the model predicts <EOS> too early or late\n",
    "        min_len = min(len(predicted_tags), len(true_tags))\n",
    "        \n",
    "        all_predicted_tags.extend(predicted_tags[:min_len])\n",
    "        all_true_tags.extend(true_tags[:min_len])\n",
    "\n",
    "    # 5. Calculate and print the results\n",
    "    accuracy = accuracy_score(all_true_tags, all_predicted_tags)\n",
    "    print(f\"\\n✅ Overall Model Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "    \n",
    "    # Generate a detailed report\n",
    "    # We get the list of all possible tags to include them in the report\n",
    "    labels = sorted(list(set(all_true_tags)))\n",
    "    report = classification_report(all_true_tags, all_predicted_tags, labels=labels)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "corrected_model.load_state_dict(model.state_dict())\n",
    "evaluate_model(corrected_model, test_data, word_to_ix, ix_to_word, ix_to_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3aade5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trained weights successfully transferred to the corrected model structure.\n",
      "\n",
      "Starting evaluation with the corrected model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1000/1000 [00:04<00:00, 222.67it/s]\n",
      "/Users/yashbhake/Desktop/IIT B/Acads/SEM-7/CS772 Deep Learning for NLP/POS-tagging-assignment-CS772/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Overall Model Accuracy: 28.78%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       0.59      0.48      0.53      2617\n",
      "         ADJ       0.31      0.02      0.03      1660\n",
      "         ADP       0.17      0.29      0.21      2691\n",
      "         ADV       0.24      0.02      0.04       925\n",
      "        CONJ       0.29      0.08      0.12       630\n",
      "         DET       0.19      0.37      0.25      2490\n",
      "        NOUN       0.34      0.41      0.37      5709\n",
      "         NUM       0.00      0.00      0.00       348\n",
      "        PRON       0.76      0.17      0.28       620\n",
      "         PRT       0.50      0.00      0.00       530\n",
      "        VERB       0.26      0.20      0.23      3271\n",
      "           X       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.29     21522\n",
      "   macro avg       0.30      0.17      0.17     21522\n",
      "weighted avg       0.32      0.29      0.27     21522\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashbhake/Desktop/IIT B/Acads/SEM-7/CS772 Deep Learning for NLP/POS-tagging-assignment-CS772/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/yashbhake/Desktop/IIT B/Acads/SEM-7/CS772 Deep Learning for NLP/POS-tagging-assignment-CS772/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Re-create the components (assuming EMBEDDING_DIM and HIDDEN_DIM are defined)\n",
    "new_encoder = Encoder(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM)\n",
    "new_decoder = Decoder(len(tag_to_ix), EMBEDDING_DIM, HIDDEN_DIM)\n",
    "corrected_model = Seq2SeqPOSTagger(new_encoder, new_decoder)\n",
    "\n",
    "# --- Step 3: Copy the trained weights from the old model to the new one ---\n",
    "# This transfers all the knowledge your model learned during training.\n",
    "\n",
    "# 'model' is the original trained model object from your training script\n",
    "corrected_model.load_state_dict(model.state_dict())\n",
    "\n",
    "print(\"✅ Trained weights successfully transferred to the corrected model structure.\")\n",
    "\n",
    "# --- Step 4: Run evaluation using the CORRECTED model ---\n",
    "# Now, call your evaluation function, but pass in 'corrected_model'\n",
    "\n",
    "print(\"\\nStarting evaluation with the corrected model...\")\n",
    "evaluate_model(corrected_model, test_data, word_to_ix, ix_to_word, ix_to_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b933bb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model state dictionary...\n",
      "✅ Model state saved to pos_tagger_model.pth\n",
      "\n",
      "Saving word_to_ix dictionary...\n",
      "✅ Word vocabulary saved to word_to_ix.json\n",
      "\n",
      "Saving tag_to_ix dictionary...\n",
      "✅ Tag vocabulary saved to tag_to_ix.json\n",
      "\n",
      "All artifacts have been saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "# --- IMPORTANT ---\n",
    "# This script assumes you have the following variables in your environment\n",
    "# after running the training code:\n",
    "# - model: The trained PyTorch model object.\n",
    "# - word_to_ix: The word-to-index dictionary.\n",
    "# - tag_to_ix: The tag-to-index dictionary.\n",
    "\n",
    "print(\"Saving model state dictionary...\")\n",
    "# We save the model's state_dict, which is just the learned weights.\n",
    "torch.save(model.state_dict(), 'pos_tagger_model.pth')\n",
    "print(\"✅ Model state saved to pos_tagger_model.pth\")\n",
    "\n",
    "print(\"\\nSaving word_to_ix dictionary...\")\n",
    "with open('word_to_ix.json', 'w') as f:\n",
    "    json.dump(word_to_ix, f)\n",
    "print(\"✅ Word vocabulary saved to word_to_ix.json\")\n",
    "\n",
    "\n",
    "print(\"\\nSaving tag_to_ix dictionary...\")\n",
    "with open('tag_to_ix.json', 'w') as f:\n",
    "    json.dump(tag_to_ix, f)\n",
    "print(\"✅ Tag vocabulary saved to tag_to_ix.json\")\n",
    "\n",
    "print(\"\\nAll artifacts have been saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
